{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoTrack.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kacsbmox36fU"
      },
      "source": [
        "\n",
        "# **AutoTrack: Spotify Song Recommendation Model Based on Facial and Image Features**\n",
        "\n",
        "*CIS545 Final Project by Navya Janga and Matthew Pearl*\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "         \n",
        "\n",
        "**Project Description**\n",
        "\n",
        "This project aims to generate a song that is fitting to a given movie clip based on actor age and emotion of the image. The overall goal of this code is to eventually create a software that will auto-generate soundtracks for entire movies. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLZF3K6d64vu"
      },
      "source": [
        "# Imports and Co-Lab Set Up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIemMmkMzTum"
      },
      "source": [
        "import os\n",
        "from os import listdir\n",
        "from google.colab import drive\n",
        "\n",
        "import gzip\n",
        "import tarfile\n",
        "import glob\n",
        "import shutil\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import tensorflow\n",
        "\n",
        "import math\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import PIL\n",
        "from PIL import Image, ImageOps\n",
        "from keras.preprocessing.image import img_to_array, load_img"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9pB1llSzPN8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27d8b911-7e47-4b4a-d378-4dae6d535403"
      },
      "source": [
        "# Using computer GPU for model training instead of colab space (****CHECKHW5*******)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Mount Google Drive -- datasets too big to load into colab directly \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AqMA-3z-0Vj"
      },
      "source": [
        "# Part 1: CNN Models for Facial Age and Emotion Detection\n",
        "\n",
        "We chose to use convolutional neural network models since they are extremely effective at image classification. The main benefit of CNNs over feed forward neural network models is that they take into account the relative positions of pixels and can be applied to images with different numbers of channels (e.g., 3 for RGB or 1 for grayscale.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLxGCOuL_mDE"
      },
      "source": [
        "## 1.1: CNN Functions \n",
        "\n",
        "Here we define a series of general CNN functions that will be implemented later in this section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgUAgYjYdZ6u"
      },
      "source": [
        "# Intializing the CNN model\n",
        "def initialize_cnn_model(in_channels, out_channels, kernel_size, stride, in_features, out_features):\n",
        "  cnn_model = torch.nn.Sequential(\n",
        "      torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.MaxPool2d(kernel_size=kernel_size-1),\n",
        "      torch.nn.Flatten(start_dim=1),\n",
        "      torch.nn.Linear(in_features=in_features, out_features=out_features)\n",
        "  )\n",
        "  return cnn_model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phCJBH1u_yOB"
      },
      "source": [
        "# training loop for CNN model\n",
        "def train_cnn_model(cnn_model, optimizer, criterion, epochs, train_loader):\n",
        "  for child in cnn_model.children():\n",
        "    if hasattr(child, 'reset_parameters'):\n",
        "      child.reset_parameters()\n",
        "  cnn_model.to(device)\n",
        "  cnn_model.train()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for data, labels in train_loader:\n",
        "      data, labels = data.to(device), labels.to(device).long()\n",
        "      outputs = cnn_model(data)\n",
        "      optimizer.zero_grad()\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "    print('epoch: {}, loss: {}'.format(epoch + 1, running_loss / len(train_loader)))\n",
        "    \n",
        "    total, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "      accuracy = correct / total\n",
        "    print('epoch: {}, accuracy: {}'.format(epoch + 1, accuracy))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOtCTe24_zea"
      },
      "source": [
        "# testing loop for CNN model\n",
        "def test_cnn_model(cnn_model, criterion, test_loader):\n",
        "  cnn_model.eval()\n",
        "  total, correct = 0, 0\n",
        "  running_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for data, labels in test_loader:\n",
        "      data, labels = data.to(device), labels.to(device).long()\n",
        "      outputs = cnn_model(data)\n",
        "      loss = criterion(outputs, labels)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "      running_loss += loss.item()\n",
        "  print('loss:', running_loss / len(test_loader), 'accuracy:', correct / total)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GDwrftY_Az5"
      },
      "source": [
        "## 1.2: Facial Age Model \n",
        "In this section, we use the UTKFace Dataset. Specifically, we are using the \"Aligned & Cropped Faces\" File. This dataset is further explored in [Appendix 1: Facial Age EDA](https://colab.research.google.com/drive/1W7CBX02pssbdHSQrHFdoU8XORCe3wxSR?usp=sharing). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "rgicRsrPLJu0",
        "outputId": "1c2ee333-b37a-4f81-b7da-0305f7362ef9"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2c711ac2-b524-4646-b96b-f4fd5c77f217\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2c711ac2-b524-4646-b96b-f4fd5c77f217\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving crop_part1.tar.gz to crop_part1.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "018cYMyEqxDa"
      },
      "source": [
        "**Test and Train Data** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtBsFLvdDbwz"
      },
      "source": [
        "!tar -xf '/content/crop_part1.tar.gz'\n",
        "\n",
        "archive = tarfile.open('/content/crop_part1.tar.gz')\n",
        "archive.extractall()\n",
        "file_names = archive.getnames()\n",
        "for i in range(len(file_names)):\n",
        "  file_names[i] = '/content/' + file_names[i]\n",
        "age_img_paths = file_names\n",
        "archive.close()\n",
        "\n",
        "# shuffle the images to create a random train and test dataset\n",
        "random.shuffle(age_img_paths)\n",
        "\n",
        "# train: 80%, test: 20%\n",
        "split_idx = int(len(age_img_paths) * 0.8)\n",
        "\n",
        "age_train_paths = age_img_paths[:split_idx]\n",
        "age_test_paths = age_img_paths[split_idx:]\n",
        "\n",
        "# function to convert an image into a 3 x 200 x 200 tensor\n",
        "def age_img_to_tensor(img_path):\n",
        "  try:\n",
        "    return torch.Tensor((np.asarray(load_img(img_path)) / 255).reshape(3, 200, 200))\n",
        "  except:\n",
        "    return torch.Tensor((np.zeros((3, 200, 200))))\n",
        "\n",
        "# return the decade of the image's subject \"teenage years\" (+15 years from birth)\n",
        "# to make a rough estimate of the type of music they listened to\n",
        "def get_decade_class_from_path(img_path):\n",
        "  try:\n",
        "    age = int(img_path.split('/')[3].split('_')[0])\n",
        "    decade = min(int(math.ceil((2021 - age + 15) / 10.0)) * 10, 2020)\n",
        "    return int((decade - 1930) / 10)\n",
        "  except:\n",
        "    return 7\n",
        "\n",
        "age_train_data_arrays = np.array([age_img_to_tensor(img_path).numpy() for img_path in age_train_paths]) \n",
        "age_train_data_labels = np.array([get_decade_class_from_path(img_path) for img_path in age_train_paths])\n",
        "\n",
        "age_test_data_arrays = np.array([age_img_to_tensor(img_path).numpy() for img_path in age_test_paths]) \n",
        "age_test_data_labels = np.array([get_decade_class_from_path(img_path) for img_path in age_test_paths])\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyZM-xxHaep-"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6fRHX08aHw2"
      },
      "source": [
        "# each dataset is composed of 3 x 200 x 200 tensors representing images from the age dataset\n",
        "age_training_dataset = TensorDataset(torch.Tensor(age_train_data_arrays), torch.Tensor(age_train_data_labels))\n",
        "age_testing_dataset = TensorDataset(torch.Tensor(age_test_data_arrays), torch.Tensor(age_test_data_labels))\n",
        "\n",
        "age_train_loader = DataLoader(age_training_dataset, batch_size=128, shuffle=True)\n",
        "age_test_loader = DataLoader(age_testing_dataset, batch_size=1, shuffle=False)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3r4czexrL1Q"
      },
      "source": [
        "**CNN Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iycycuS8d2Hb"
      },
      "source": [
        "# age CNN model parameters\n",
        "age_in_channels = 3 # for the 3 RGB channels\n",
        "age_out_channels = 64\n",
        "age_kernel_size = 4\n",
        "age_stride = 2\n",
        "age_in_features = 33 * 33 * 64 # from the convnet calculator based on the chosen kernel, stride and out_channels sizes\n",
        "age_out_features = len(set(age_train_data_labels).union(set(age_test_data_labels))) # number of possible classes\n",
        "\n",
        "age_cnn_model = initialize_cnn_model(age_in_channels, age_out_channels, age_kernel_size, age_stride, age_in_features, age_out_features)\n",
        "\n",
        "# we are using a cross entropy loss function and an Adam optimizer with a learning rate of 1e-3\n",
        "age_criterion = torch.nn.CrossEntropyLoss()\n",
        "age_optimizer = torch.optim.Adam(age_cnn_model.parameters(), lr=1e-3)\n",
        "\n",
        "age_epochs = 15"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsvaSJA3fqaR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9079b9df-50e4-43be-fd8c-479ecf6a2f6d"
      },
      "source": [
        "train_cnn_model(age_cnn_model, age_optimizer, age_criterion, age_epochs, age_train_loader)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1, loss: 3.2231777791054017\n",
            "epoch: 1, accuracy: 0.5\n",
            "epoch: 2, loss: 1.4672459671574254\n",
            "epoch: 2, accuracy: 0.625\n",
            "epoch: 3, loss: 1.279827279429282\n",
            "epoch: 3, accuracy: 0.5625\n",
            "epoch: 4, loss: 1.1930627803648672\n",
            "epoch: 4, accuracy: 0.5625\n",
            "epoch: 5, loss: 1.1485433347763554\n",
            "epoch: 5, accuracy: 0.5625\n",
            "epoch: 6, loss: 1.106885515874432\n",
            "epoch: 6, accuracy: 0.625\n",
            "epoch: 7, loss: 1.0807066425200431\n",
            "epoch: 7, accuracy: 0.5\n",
            "epoch: 8, loss: 1.0289070231299247\n",
            "epoch: 8, accuracy: 0.625\n",
            "epoch: 9, loss: 1.0094760944766383\n",
            "epoch: 9, accuracy: 0.375\n",
            "epoch: 10, loss: 0.9519179357636359\n",
            "epoch: 10, accuracy: 0.625\n",
            "epoch: 11, loss: 0.9183795067571825\n",
            "epoch: 11, accuracy: 0.75\n",
            "epoch: 12, loss: 0.8687595744286815\n",
            "epoch: 12, accuracy: 0.6875\n",
            "epoch: 13, loss: 0.8125473674266569\n",
            "epoch: 13, accuracy: 0.625\n",
            "epoch: 14, loss: 0.7871734672977079\n",
            "epoch: 14, accuracy: 0.75\n",
            "epoch: 15, loss: 0.7444342682438512\n",
            "epoch: 15, accuracy: 0.625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhOUs228AyPc"
      },
      "source": [
        "torch.save(age_cnn_model.state_dict(), 'age_checkpoint.pth')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRRyavi00X1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a76401a-ca4b-42fa-8e18-365687a16b06"
      },
      "source": [
        "age_criterion = torch.nn.CrossEntropyLoss()\n",
        "test_cnn_model(age_cnn_model, age_criterion, age_test_loader)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 1.280514815858302 accuracy: 0.575370464997445\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WuROerQ9eyf"
      },
      "source": [
        "## 1.3: Facial Emotion Model\n",
        "\n",
        "This section creates a model for the Facial Emotion dataset, which is further explored in Appendix 2: Facial Emotion Model. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_LVma7UrUGu"
      },
      "source": [
        "**Test and Train Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_coqYFd9HpRB"
      },
      "source": [
        "# since the grayscale pictures come as strings of pixel values, we need to convert\n",
        "# these strings into a tensor\n",
        "def get_pixel_tensor(pixel_str):\n",
        "  return torch.Tensor([float(s) / 255 for s in pixel_str.split(' ')])\n",
        "\n",
        "# read in the ICML face data as a Pandas dataframe\n",
        "emotions_df = pd.read_csv('/content/drive/Shared drives/CIS545 Final Project/icml_face_data.csv', skip_blank_lines=False)\n",
        "emotions_df[' pixels'] = emotions_df[' pixels'].apply(lambda pixel_str: get_pixel_tensor(pixel_str))\n",
        "\n",
        "# 2 separate dataframes for training and testing\n",
        "emotions_train_df = emotions_df[emotions_df[' Usage'] == 'Training']\n",
        "emotions_test_df = emotions_df[emotions_df[' Usage'] == 'PrivateTest']\n",
        "\n",
        "# generate training and testing data\n",
        "# the inputs are 1 x 48 x 48 tensors representing the images in the dataset\n",
        "# the outputs are integer encodings of the emotion represented in the images\n",
        "emotions_train_arrays = np.array([np.resize(t.numpy(), (1, 48, 48)) for t in emotions_train_df[' pixels'].to_list()])\n",
        "emotions_train_emotions = np.array(emotions_train_df['emotion'].tolist())\n",
        "\n",
        "emotions_test_arrays = np.array([np.resize(t.numpy(), (1, 48, 48)) for t in emotions_test_df[' pixels'].to_list()])\n",
        "emotions_test_emotions = np.array(emotions_test_df['emotion'].tolist())"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWuBdgxBdTXO"
      },
      "source": [
        "# generate PyTorch datasets and data loaders for training/testing based on the previously acquired data\n",
        "emotions_training_dataset = TensorDataset(torch.Tensor(emotions_train_arrays), torch.Tensor(emotions_train_emotions))\n",
        "emotions_testing_dataset = TensorDataset(torch.Tensor(emotions_test_arrays), torch.Tensor(emotions_test_emotions))\n",
        "\n",
        "emotions_train_loader = DataLoader(emotions_training_dataset, batch_size=128, shuffle=True)\n",
        "emotions_test_loader = DataLoader(emotions_testing_dataset, batch_size=1, shuffle=False)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMtCtVHmrcVb"
      },
      "source": [
        "**CNN Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTB_DY7U8aOO"
      },
      "source": [
        "# emotion CNN parameters\n",
        "emotions_in_channels = 1 # 1 channel because the image is grayscale\n",
        "emotions_out_channels = 12\n",
        "emotions_kernel_size = 2\n",
        "emotions_stride = 1\n",
        "emotions_in_features = 26508 # from the convnet calculator\n",
        "emotions_out_features = 7 # number of possible emotion classes\n",
        "\n",
        "# we are using a cross entropy loss function and an Adam optimizer with a learning rate of 1e-3\n",
        "emotions_cnn_model = initialize_cnn_model(emotions_in_channels, emotions_out_channels, emotions_kernel_size, emotions_stride, emotions_in_features, emotions_out_features)\n",
        "emotions_criterion = torch.nn.CrossEntropyLoss()\n",
        "emotions_optimizer = torch.optim.Adam(emotions_cnn_model.parameters(), lr=1e-3)\n",
        "\n",
        "emotions_epochs = 15"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnKLWiz99Vvx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7e181b2-5f94-4f6b-b5fd-0110c7075de9"
      },
      "source": [
        "train_cnn_model(emotions_cnn_model, emotions_optimizer, emotions_criterion, emotions_epochs, emotions_train_loader)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1, loss: 1.8205373403761123\n",
            "epoch: 1, accuracy: 0.3783783783783784\n",
            "epoch: 2, loss: 1.616312017440796\n",
            "epoch: 2, accuracy: 0.35135135135135137\n",
            "epoch: 3, loss: 1.5494705756505331\n",
            "epoch: 3, accuracy: 0.3783783783783784\n",
            "epoch: 4, loss: 1.5068836238649157\n",
            "epoch: 4, accuracy: 0.43243243243243246\n",
            "epoch: 5, loss: 1.4679030270046658\n",
            "epoch: 5, accuracy: 0.5675675675675675\n",
            "epoch: 6, loss: 1.4352569823794894\n",
            "epoch: 6, accuracy: 0.4864864864864865\n",
            "epoch: 7, loss: 1.4144621430502997\n",
            "epoch: 7, accuracy: 0.24324324324324326\n",
            "epoch: 8, loss: 1.3888413344489203\n",
            "epoch: 8, accuracy: 0.32432432432432434\n",
            "epoch: 9, loss: 1.3635575803120932\n",
            "epoch: 9, accuracy: 0.4594594594594595\n",
            "epoch: 10, loss: 1.3388655461205377\n",
            "epoch: 10, accuracy: 0.4864864864864865\n",
            "epoch: 11, loss: 1.3262824508878919\n",
            "epoch: 11, accuracy: 0.5405405405405406\n",
            "epoch: 12, loss: 1.2968372270796034\n",
            "epoch: 12, accuracy: 0.4594594594594595\n",
            "epoch: 13, loss: 1.2821579684151543\n",
            "epoch: 13, accuracy: 0.43243243243243246\n",
            "epoch: 14, loss: 1.252694690492418\n",
            "epoch: 14, accuracy: 0.5405405405405406\n",
            "epoch: 15, loss: 1.2371570348739624\n",
            "epoch: 15, accuracy: 0.5945945945945946\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgNuuzLjrgXB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "872b5e7c-1e69-43b1-c300-690ea4853133"
      },
      "source": [
        "emotions_criterion = torch.nn.CrossEntropyLoss()\n",
        "test_cnn_model(emotions_cnn_model, emotions_criterion, emotions_test_loader)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 1.5343169760920148 accuracy: 0.4134856505990527\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyKlwmu--A0R"
      },
      "source": [
        "# Part 2: Music Selection for Video Clips"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zLcW2MuRcU3"
      },
      "source": [
        "## Part 2.1: Get Spotify Data Ready"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XcX3hLbdhRc"
      },
      "source": [
        "In this section, we will take out Spotify Dataset, merge it with genre, and take the 2000 most popular songs. The EDA and cleaning for this section can be found in [Appendix 3: Spotify EDA](https://colab.research.google.com/drive/1HwJVahChbeW6yxtr92s2bqAlvHkMuCzA?usp=sharing). \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-aFMNpoezoe"
      },
      "source": [
        "# read in the spotify data as a Pandas dataframe\n",
        "path_to_spotify_data = '/content/drive/Shared drives/CIS545 Final Project/spotify_data.csv'\n",
        "spotify_df = pd.read_csv(path_to_spotify_data, error_bad_lines=False).dropna().drop_duplicates()\n",
        "\n",
        "# convert any numerical features to a numeric type\n",
        "cols_to_numeric = ['acousticness', 'danceability', 'duration_ms', 'energy', 'explicit', 'instrumentalness', 'key', 'liveness', 'loudness', 'mode', 'popularity', 'year']\n",
        "spotify_df[cols_to_numeric] = spotify_df[cols_to_numeric].apply(pd.to_numeric)\n",
        "\n",
        "# utility function to convert a song's year to the decade it was released\n",
        "def year_to_decade(year):\n",
        "  return year - (year % 10)\n",
        "\n",
        "# apply the utility function to the dataframe\n",
        "spotify_df['year'] = spotify_df['year'].apply(lambda y: year_to_decade(y))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GUAA8ah9p98"
      },
      "source": [
        "# read in the top two thousand songs data as a Pandas dataframe\n",
        "path_to_top_two_thousand = '/content/drive/Shared drives/CIS545 Final Project/Spotify-2000.csv'\n",
        "df_top_two_thousand = pd.read_csv(path_to_top_two_thousand).dropna().drop_duplicates()[['Title', 'Artist', 'Top Genre']]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQD2Eiz890-N"
      },
      "source": [
        "# a list of more general genres to simplify the two thousand top songs data\n",
        "general_genres = ['rock', 'jazz', 'hip hop', 'pop', 'alternative', 'adult standards', 'folk', 'indie', 'metal']"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T04ZXBm92is"
      },
      "source": [
        "# given a genre in the actual dataframe, we convert it to a general genre if the name \n",
        "# of the general genre appears in that of the actual - otherwise, we return 'other'\n",
        "def convert_to_generic_genre(genre):\n",
        "  for g in general_genres:\n",
        "    if g in genre:\n",
        "      return g\n",
        "  return 'other'\n",
        "\n",
        "# apply the function to the dataframe in order to only have the more general genres\n",
        "df_top_two_thousand['Top Genre'] = df_top_two_thousand['Top Genre'].apply(lambda g: convert_to_generic_genre(g))\n",
        "df_top_two_thousand['Artist'] = df_top_two_thousand['Artist'].apply(lambda a: [a])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfCX010I-WPi"
      },
      "source": [
        "# a mapping from the general genres to an integer\n",
        "genre_encoding = {}\n",
        "\n",
        "for i in range(len(general_genres)):\n",
        "  genre_encoding.update({general_genres[i]: i})\n",
        "genre_encoding.update({'other': 9})"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8gcID0Z98d3"
      },
      "source": [
        "# merge the top two thousand songs dataframe on the original spotify dataframe\n",
        "merged_df = df_top_two_thousand.merge(spotify_df, left_on='Title', right_on='name').drop_duplicates(subset=['Title'])\n",
        "merged_df = merged_df.drop(['name'], axis=1)\n",
        "merged_df['genre encoding'] = merged_df['Top Genre'].apply(lambda g: genre_encoding.get(g))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ue9AvrNWVtb"
      },
      "source": [
        "## Part 2.2: Video Clip Information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwiNLSzvdggU"
      },
      "source": [
        "from PIL import Image, ImageOps\n",
        "import cv2\n",
        "from collections import defaultdict\n",
        "import operator"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-qKekt53pgb"
      },
      "source": [
        "emotions_map = {\n",
        "  0: 'Angry', \n",
        "  1: 'Disgust', \n",
        "  2: 'Fear', \n",
        "  3: 'Happy', \n",
        "  4: 'Sad', \n",
        "  5: 'Surprise', \n",
        "  6: 'Neutral'\n",
        "}"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FctORuvCmdFO"
      },
      "source": [
        "This is the crux of how our CNN models are applied to a movie clip. We begin by looping through each frame in the given mp4 file and consider those with a face using opencv's facial detection functionality. For each frame with a face, we predict the movie character's age and emotion and create a distribution for both. Finally, we return the age and emotion prediction with the maximum frequency. Given an approximately 45% accuracy for emotion detection and 55% accuracy for age determination, we can expect that for a clip with a sufficient amount of frames with faces, the majority of the distribution will often be correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0NPTb3Q_zgp"
      },
      "source": [
        "def get_clip_info(path_to_clip):\n",
        "  emotion_tensors = []\n",
        "  age_tensors = []\n",
        "\n",
        "  face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "  capture = cv2.VideoCapture(path_to_clip)\n",
        "  success, image = capture.read()\n",
        "  while success:\n",
        "    try:\n",
        "      # read in the image from the frame, convert to grayscale, and detect faces\n",
        "      success, image = capture.read()\n",
        "      image_grayscale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "      faces = face_cascade.detectMultiScale(image_grayscale, 1.1, 4)\n",
        "      # only focus on frames where 1 face is detected\n",
        "      if len(faces) == 1:\n",
        "        for (x, y, w, h) in faces:\n",
        "          cv2.rectangle(image, (x, y), (x+w, y+h), (0, 0, 0), 2)\n",
        "        # create a 1 x 48 x 48 tensor representation of the image for the emotion CNN\n",
        "        emotion_img = ImageOps.grayscale(Image.fromarray(image[y:y+w, x:x+h], 'RGB')).resize((48, 48))\n",
        "        emotion_tensor = torch.Tensor(np.array(emotion_img).reshape(1, 48, 48) / 255)\n",
        "        emotion_tensors.append(emotion_tensor)\n",
        "        # create a 3 x 200 x 200 tensor representation of the image for the age CNN\n",
        "        age_img = Image.fromarray(image[y:y+w, x:x+h], 'RGB').resize((200, 200))\n",
        "        age_tensor = torch.Tensor(np.array(age_img).reshape(3, 200, 200) / 255)\n",
        "        age_tensors.append(age_tensor)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  # create a distribution of the predicted emotion and decade in the entire clip\n",
        "  emotion_frequencies = defaultdict( int )\n",
        "  age_frequencies = defaultdict( int )\n",
        "\n",
        "  for tensor in emotion_tensors:\n",
        "    outputs = emotions_cnn_model(tensor.unsqueeze(0))\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    emotion_frequencies[predicted.item()] += 1\n",
        "  \n",
        "  for tensor in age_tensors:\n",
        "    outputs = age_cnn_model(tensor.unsqueeze(0))\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    age_frequencies[predicted.item()] += 1\n",
        "\n",
        "  # the returned predicted emotion and decade will be those with the highest frequency\n",
        "  # assuming both models' accuracy is around ~45-60%, we can expect this to be pretty\n",
        "  # accurate for a sufficient number of frames with faces\n",
        "  predicted_emotion = max(emotion_frequencies.items(), key=operator.itemgetter(1))[0]\n",
        "  predicted_decade = max(age_frequencies.items(), key=operator.itemgetter(1))[0]\n",
        "\n",
        "  print({'emotion encoding': predicted_emotion, 'emotion': emotions_map.get(predicted_emotion), 'music decade': predicted_decade * 10 + 1930})\n",
        "  return {'emotion encoding': predicted_emotion, 'emotion': emotions_map.get(predicted_emotion), 'music decade': predicted_decade * 10 + 1930}"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOxL017rRQqr"
      },
      "source": [
        "## Part 2.3 Song Selection "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo3aEkTRnJlS"
      },
      "source": [
        "This section contains the functionality to make a song selection given the information returned from analyzing the frames of the movie clip. We begin by simply filtering the spotify dataset to songs released in the predicted decade for the character in the movie clip. Next, upon researching how songs and emotion relate, we made decisions about how to further narrow down the dataset based on the following rules: <br><br>\n",
        "\n",
        "sad songs: low valence, low tempo <br>\n",
        "happy/surprise songs: high valence, high tempo <br>\n",
        "angry/disgust/fear songs: songs in the metal genre <br>\n",
        "neutral songs: 0.45 <= valence <= 0.55, low danceability <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nPiDNl_CN2n"
      },
      "source": [
        "# to view all dataframe columns when printed (used for testing)\n",
        "pd.options.display.max_columns = None"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzPVJ2JL_-_Y"
      },
      "source": [
        "# based on the model predictions and selected song, returns a string for user interaction\n",
        "def get_song_string(title, artist, clip_info_dict, has_decade):\n",
        "  age_str = 'and based on their age probably likes music from the {}s.'.format(clip_info_dict.get('music decade')) if has_decade else 'The character would probably like music from the {}s but there are no songs from this decade'.format(clip_info_dict.get('music decade'))\n",
        "  return \"AutoTrack suggests {} by {} for the clip you input because the character's emotion appears to be {}\\n\".format(title, artist, clip_info_dict.get('emotion')) + age_str\n",
        "\n",
        "# given the predictions from the CNN models, this function implements the previously\n",
        "# discussed rules and, once filtered, picks a random song from the remaining dataframe\n",
        "def select_song(clip_info_dict):\n",
        "  music_decade = clip_info_dict.get('music decade')\n",
        "  emotion = clip_info_dict.get('emotion')\n",
        "\n",
        "  if emotion == 'Sad':\n",
        "    has_decade = False\n",
        "    sad_df = merged_df.sort_values(by=['valence'])\n",
        "    if len(sad_df[sad_df['year'] == music_decade]) > 0:\n",
        "      sad_df = sad_df[sad_df['year'] == music_decade]\n",
        "      has_decade = True\n",
        "    sad_df = sad_df.sort_values(by=['tempo']).head(min(20, len(sad_df)))\n",
        "    max_idx = min(20, len(sad_df))\n",
        "    rand_idx = random.randrange(0, max_idx)\n",
        "    title = sad_df.iloc[rand_idx]['Title']\n",
        "    artist = sad_df.iloc[rand_idx]['Artist'][0]\n",
        "    return get_song_string(title, artist, clip_info_dict, has_decade)\n",
        "  \n",
        "  elif emotion == 'Happy' or emotion == 'Surprise':\n",
        "    has_decade = False\n",
        "    happy_df = merged_df[(merged_df['Top Genre'] == 'pop') & (merged_df['mode'] == 1)].sort_values(by=['valence'], ascending=False)\n",
        "    if len(happy_df[happy_df['year'] == music_decade]) > 0:\n",
        "      happy_df = happy_df[happy_df['year'] == music_decade]\n",
        "      has_decade = True\n",
        "    happy_df = happy_df.sort_values(by=['tempo'], ascending=False).head(min(20, len(happy_df)))\n",
        "    max_idx = min(20, len(happy_df))\n",
        "    rand_idx = random.randrange(0, max_idx)\n",
        "    title = happy_df.iloc[rand_idx]['Title']\n",
        "    artist = happy_df.iloc[rand_idx]['Artist'][0]\n",
        "    return get_song_string(title, artist, clip_info_dict, has_decade)\n",
        "\n",
        "  elif emotion == 'Angry' or emotion == 'Disgust' or emotion == 'Fear':\n",
        "    has_decade = False\n",
        "    metal_df = merged_df[merged_df['Top Genre'] == 'metal']\n",
        "    if len(metal_df[metal_df['year'] == music_decade]) > 0:\n",
        "      metal_df = metal_df[metal_df['year'] == music_decade]\n",
        "      has_decade = True\n",
        "    max_idx = min(20, len(metal_df))\n",
        "    rand_idx = random.randrange(0, max_idx)\n",
        "    title = metal_df.iloc[rand_idx]['Title']\n",
        "    artist = metal_df.iloc[rand_idx]['Artist'][0]\n",
        "    return get_song_string(title, artist, clip_info_dict, has_decade)\n",
        "  \n",
        "  elif emotion == 'Neutral':\n",
        "    has_decade = False\n",
        "    neutral_df = merged_df[(merged_df['valence'] >= 0.45) & (merged_df['valence'] <= 0.55)].sort_values(by=['danceability'])\n",
        "    if len(neutral_df[neutral_df['year'] == music_decade]) > 0:\n",
        "      neutral_df = neutral_df[neutral_df['year'] == music_decade]\n",
        "      has_decade = True\n",
        "    max_idx = min(20, len(neutral_df))\n",
        "    rand_idx = random.randrange(0, max_idx)\n",
        "    title = metal_df.iloc[rand_idx]['Title']\n",
        "    artist = metal_df.iloc[rand_idx]['Artist'][0]\n",
        "    return get_song_string(title, artist, clip_info_dict, has_decade)\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ya3hNiTZNnN"
      },
      "source": [
        "# the main function - given a path to an mp4 file, AutoTrack will pair it with a song\n",
        "def pick_song(path_to_clip):\n",
        "  clip_info = get_clip_info(path_to_clip)\n",
        "  return select_song(clip_info)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLKhF0Yfkjd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72869e19-d9a2-4a9e-bca9-55b42f7f8080"
      },
      "source": [
        "path_to_clip = 'giphy_7.mp4' # ENTER YOUR MP4 PATH HERE\n",
        "print(pick_song(path_to_clip))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'emotion encoding': 4, 'emotion': 'Sad', 'music decade': 2020}\n",
            "AutoTrack suggests Nine Million Bicycles by Katie Melua for the clip you input because the character's emotion appears to be Sad\n",
            "and based on their age probably likes music from the 2020s.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO-XfLSiP8-X"
      },
      "source": [
        "# Part 3: AutoTrack in Action\n",
        "\n",
        "Here we have displayed results of AutoTrack on two GIFs that we ran through the code. In order for AutoTrack to work, the input GIF must be in MP4 format. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwKvmmu9QQjA"
      },
      "source": [
        "## Part 3.1 Happy GIF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y07H6X51QZAx"
      },
      "source": [
        "![](https://media.giphy.com/media/3og0ICmyySyzbmnxqE/giphy.gif)\n",
        "\n",
        "{'emotion encoding': 3, 'emotion': 'Happy', 'music decade': 1990}<br>\n",
        "AutoTrack suggests Man in the Mirror by Michael Jackson for the clip you input because the character's emotion appears to be Happy<br>\n",
        "and based on their age probably likes music from the 1990s.<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sqIaEqjQU1o"
      },
      "source": [
        "## Part 3.2 Sad GIF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDWyLRWdFj9I"
      },
      "source": [
        "\n",
        "![](https://media.giphy.com/media/hmaIFbMUjVaZG/giphy.gif)\n",
        "\n",
        "{'emotion encoding': 4, 'emotion': 'Sad', 'music decade': 2020}<br>\n",
        "AutoTrack suggests Nine Million Bicycles by Katie Melua for the clip you input because the character's emotion appears to be Sad<br>\n",
        "and based on their age probably likes music from the 2020s.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed-8X-9hiX2l"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **Appendix**\n",
        "This contains hyperlinks of Google Colabs that explore the datasets we used including EDA and testing. \n",
        "\n",
        "[1: Facial Age EDA](https://colab.research.google.com/drive/1W7CBX02pssbdHSQrHFdoU8XORCe3wxSR?usp=sharing)\n",
        "\n",
        "[2: Facial Emotion EDA](https://colab.research.google.com/drive/1zUTpqxVV3NBYfWjwEkHQwvxiAFpV-uqU?usp=sharing)\n",
        "\n",
        "[3: Spotify EDA](https://colab.research.google.com/drive/1HwJVahChbeW6yxtr92s2bqAlvHkMuCzA?usp=sharing)\n"
      ]
    }
  ]
}